{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww16180\viewh12960\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Scoring Algorithm, backup\
\
Functions for file I/O\
\
def importLabels(filenameString):\
    import csv\
    import numpy as np\
    #Import manual scores\
    with open(filenameString, newline='') as csvfile:\
        manualScores = list(csv.reader(csvfile))\
    #Initialize\
    labels = []\
    l = len(manualScores) - 2\
\
    #Replaces all blank scores with zeros \
    count = 0\
    for i in manualScores:\
        count = count + 1\
        if count > 2:\
            if i[1] == '':\
                i[1] = 0\
\
    #Create separate list of scores\
    for j in range(l):\
        labels.append(manualScores[j+2][1])\
    return labels\
\
def importPathnames(directoryString):\
    import os\
    pathnames = []\
    folders = []\
    subfolders1 = []\
    subfolders2 = []\
    filenames_direct = []\
    filenames_left = []\
    scoresheets = []\
    #directory1 = os.fsencode(directoryString)\
    for file in os.listdir(directoryString):\
        folder = os.fsdecode(file)\
        if folder[0:2] == 'R0':\
            folders.append(folder)\
    #Loop through each rat folder\
    for folder in folders:\
        #print("new folder")\
        #directory2 = os.fsencode(folder)\
        #print(directory2)\
        #Loop through each day of testing\
        for subfolder1 in os.listdir(directoryString+'/'+folder):\
            subfolder1 = os.fsdecode(subfolder1)\
            if (not subfolder1.endswith('.fig')) and (not subfolder1.endswith('.pdf')) and (not 'temp' in subfolder1):\
                if '2017' in subfolder1 or '2018' in subfolder1:\
                    subfolders1.append(subfolder1)\
                    for subfolder2 in os.listdir(directoryString+'/'+folder+'/'+subfolder1):\
                        subfolder2 = os.fsdecode(subfolder2)\
                        #Loop through CSV files for direct view\
                        if 'direct' in subfolder2:\
                            subfolders2.append(subfolder2)\
                            for file in os.listdir(directoryString+'/'+folder+'/'+subfolder1+'/'+subfolder2):\
                                filename_direct = os.fsdecode(file)\
                                filenames_direct.append(filename_direct)\
                        #Loop through CSV files for left view\
                        if 'left' in subfolder2:\
                            subfolders2.append(subfolder2)\
                            for file in os.listdir(directoryString+'/'+folder+'/'+subfolder1+'/'+subfolder2):\
                                filename_left = os.fsdecode(file)\
                                filenames_left.append(filename_left)\
            if 'score' in subfolder1:\
                scoresheets.append(subfolder2)                            \
        #print(scoresheets)\
            #Bubblesort to get pathnames in order\
            #n = len(filenames) \
            # Traverse through all array elements\
            #for i in range(n): \
            # Last i elements are already in place \
                #for j in range(0, n-i-1): \
                    #sub1 = list(filenames[j])\
                    #sub2 = list(filenames[j+1])\
                    # traverse the array from 0 to n-i-1 \
                    # Swap if the element found is greater \
                    # than the next element \
                    #if sub1[24:27] > sub2[24:27] : \
                        #filenames[j], filenames[j+1] = filenames[j+1], filenames[j]\
    for d in folders:\
        for e in subfolders1: \
            for f in subfolders2:\
                for g  in filenames_direct:\
                    pathname_d = directoryString+'/'+str(d)+'/'+str(e)+'/'+str(f)+'/'+str(g)\
                    print(pathname_d)\
                    pathnames.append(pathname_d) \
                    for h in filenames_left:\
                        pathname_l = directoryString+'/'+str(d)+'/'+str(e)+'/'+str(f)+'/'+str(h)\
                        print(pathname_l)\
                        pathnames.append(pathname_l) \
    return pathnames\
\
pathnames = importPathnames('Rats')\
#print(pathnames)\
\
def importFeatures(pathnames,labels):\
    #Imports features from csv files\
    features = []\
    #Get list of file names\
    for j in pathnames:\
        with open(str(j), newline='') as csvfile:\
                kinematics = list(csv.reader(csvfile))\
                kinematics = np.asarray(kinematics)\
                features.append(kinematics[3:,1:])\
    print(np.shape(features))\
    labels = labels[0:len(features)]\
    return features, labels\
\
def scaleProb(features):\
    #Scale all coordinates by associated probability, save new features\
    #matrix as pScaledFeatures\
    \
    #INSTEAD: Go through data and cut out anything with a probability of <.90?\
    #But there will be entire frames where several but not all points have a\
    #high probability -- instead, omit frames where the cumulative probability of all points is low\
    pScaledFeatures = features\
    temp = np.empty([73,1291,32])\
    #All videos\
    icount = 0\
    for i in pScaledFeatures: \
        jcount = 0\
        #All frames\
        for j in i:\
            kcount = 0\
            sz = len(j)\
            #All features\
            sumprob = 0\
            for k in range(sz-1):\
                if ((k+1)%3) == 0:\
                    if float(j[k]) < 0.9:\
                        #print(float(j[k]))\
                        j[k-1] = 0\
                        j[k-2] = 0\
                    else:\
                        print(float(j[k]))\
                    #kcount = kcount+2\
            #jcount = jcount + 1\
            #print(np.shape(temp))\
        #icount = icount + 1\
    return temp\
\
#temp = scaleProb(features)\
#print(np.shape(features))\
#print(np.shape(temp))\
\
temp3 = np.empty([6,6,6])\
n, r, c = np.shape(temp3)\
for i in range(n):\
    for j in range(r):\
        for k in range(c):\
            temp3[i,j,k] = j\
            \
\
#print(temp3)\
def deleteRow(temp3):\
    n, r, c = np.shape(temp3)\
    print(temp3)\
    total= 0\
    for i in range(n):\
        #print(n)\
        #print(r)\
        #rint(c)\
        for j in range(r):\
            for k in range(c):\
                total= total + temp3[i,j,k]\
            print(total)\
            if total >18:\
                #print(i)\
                #print(j)\
                #print(k)\
                #temp3 = np.delete(temp3,j,1)\
                print(np.shape(temp3))\
                #temp3 = deleteRow(temp3) \
            total = 0\
    return temp3\
\
temp3 = deleteRow(temp3)\
\
def normalize(pScaledFeatures):\
    #Scale all coordinates by max pixel value\
    maxPix = 551\
\
    normalizedFeatures = pScaledFeatures\
\
    for a in normalizedFeatures:\
        for b in a:\
            sz = len(b)\
            for c in range(sz-1):\
                b[c] = float(b[c])/551\
    return normalizedFeatures\
\
Workspace\
\
filenameString = 'R0186_20170815a_manualScores.csv'\
labels = importLabels(filenameString)\
directoryString = 'sampleData/R0186_20170815a/R0186_20170815a_direct/'\
pathnames = importPathnames(directoryString)\
features,labels = importFeatures(pathnames,labels)\
#pScaledFeatures = scaleProb(features)\
normalizedFeatures = normalize(features)\
#print(normalizedFeatures)\
\
#Do PCA on kinematics data\
from sklearn.decomposition import PCA\
\
redDimFeatures = np.empty([73,32])\
x = normalizedFeatures[0].transpose()\
x = np.asarray(x, dtype=float)\
pca = PCA(n_components=32)\
pca.fit(x)\
x = pca.transform(x)\
ycount = 0\
for y in normalizedFeatures:\
    y = y.transpose()\
    y = np.asarray(y, dtype=float)\
    redDimFeatures[ycount] = pca.transform(y)[0] \
    ycount= ycount+1\
\
from sklearn.neural_network import MLPClassifier as MLP\
\
#Train a random forest\
clf = MLP(solver='sgd', max_iter=10000)\
clf.fit(redDimFeatures, labels)\
\
testFilenameString = 'R0189_20170924a_manualScores.csv'\
testlabels = importLabels(testFilenameString)\
directoryString = 'sampleData/R0189_20170924a/R0189_20170924a_direct/'\
pathnames = importPathnames(directoryString)\
testfeatures,testlabels = importFeatures(testpathnames,testlabels)\
#testpScaledFeatures = scaleProb(testfeatures)\
testNormalizedFeatures = normalize(testfeatures)\
testRedDim = np.empty([testl,32])\
ncount = 0\
for n in testNormalizedFeatures:\
    n = n.transpose()\
    n = np.asarray(n,dtype=float)\
    testRedDim[ncount] = pca.transform(n)[0]\
    ncount = ncount +1\
testOutput = clf.predict(testRedDim)\
\
trainingOutput = clf.predict(redDimFeatures)\
print(output)\
print(np.shape(output))\
\
points = 0\
for i in range(len(labels)):\
    if labels[i] == trainingOutput [i]:\
        points = points + 1\
        \
print(points)\
print(points/len(labels))\
\
filenameString = 'R0189_20170924a_manualScores.csv'\
labels = importLabels(filenameString)\
vid, fram, feat = np.shape(normalizedFeatures)\
directoryString = 'sampleData/R0189_20170924a/R0189_20170924a_direct/'\
pathnames = importPathnames(directoryString)\
features,labels = importFeatures(pathnames,labels)\
#pScaledFeatures = scaleProb(features)\
normalizedFeatures = normalize(features)\
\
redDimFeatures = np.empty([vid,32])\
x = normalizedFeatures[0].transpose()\
x = np.asarray(x, dtype=float)\
pca2 = PCA(n_components=32)\
pca2.fit(x)\
x = pca2.transform(x)\
ycount = 0\
for y in normalizedFeatures:\
    y = y.transpose()\
    y = np.asarray(y, dtype=float)\
    redDimFeatures[ycount] = pca2.transform(y)[0] \
    ycount= ycount+1\
    \
clf2 = MLP(solver='sgd', max_iter=10000)\
clf2.fit(redDimFeatures, labels)\
\
testFilenameString = 'R0186_20170815a_manualScores.csv'\
testlabels = importLabels(testFilenameString)\
vid2, fram2, feat2 = np.shape(normalizedFeatures)\
directoryString = 'sampleData/R0186_20170815a/R0186_20170815a_direct/'\
pathnames = importPathnames(directoryString)\
testfeatures,testlabels = importFeatures(testpathnames,testlabels)\
#testpScaledFeatures = scaleProb(testfeatures)\
testNormalizedFeatures = normalize(testfeatures)\
testRedDim = np.empty([vid2,32])\
ncount = 0\
for n in testNormalizedFeatures:\
    n = n.transpose()\
    n = np.asarray(n,dtype=float)\
    testRedDim[ncount] = pca2.transform(n)[0]\
    ncount = ncount +1\
testOutput = clf2.predict(testRedDim)\
\
print(testlabels)\
\
points = 0\
for i in range(len(testlabels)):\
    if int(testlabels[i]) == int(testOutput [i]):\
        points = points + 1\
        \
print(points)\
print(points/len(testlabels))\
\
trainingOutput = clf2.predict(redDimFeatures)\
print(output)\
print(np.shape(output))\
\
print(labels)\
print(np.shape(labels))\
\
points = 0\
for i in range(len(labels)):\
    if int(labels[i]) == int(trainingOutput[i]):\
        points = points + 1\
        \
print(points)\
print(points/len(labels))}